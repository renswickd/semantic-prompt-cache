# RAG + Semantic Cache System

This project is designed to enhance a Retrieval-Augmented Generation (RAG) pipeline with a custom-built Semantic Cache system. The primary goal is to reduce redundant LLM (Large Language Model) calls, improve system responsiveness, and optimize cost for real-time and large-scale AI applications.

## ğŸš€ Purpose

In traditional RAG pipelines, every user query is processed through document retrieval and LLM generationâ€”even if a semantically similar query was already answered. This approach increases latency and inflates API usage costs.

This system introduces a semantic caching layer that intercepts incoming queries and compares themâ€”based on meaning, not just keywordsâ€”against previously answered queries. If a sufficiently similar query is found, the cached response is reused, bypassing the need for another LLM call.

## ğŸ”§ Use Cases

- **Chatbots with memory efficiency**  
  Minimize repeated LLM calls for frequently asked or rephrased questions.

- **Enterprise knowledge assistants**  
  Provide consistent and faster answers to similar user queries across departments.

- **High-throughput RAG pipelines**  
  Scale to thousands of queries per day while maintaining performance and reducing cost.

- **Latency-sensitive applications**  
  Reduce end-user wait time by short-circuiting the full RAG flow when a cached response is available.


# Semantic Cache for LLM-Enhanced RAG

A modular, non-OOP semantic caching system built to reduce LLM calls and latency in Retrieval-Augmented Generation (RAG) pipelines.

## ğŸ”§ Features

- âœ… Embeds user queries using `bge-small-en-v1.5`
- âœ… Stores query-response pairs with FAISS index
- âœ… Retrieves cached results based on semantic similarity
- âœ… Configurable similarity threshold
- âœ… Supports metadata (timestamps, hits) and leaderboard extensions
- âœ… Fully functional with Mistral (via Groq) or any OpenRouter-compatible LLM
- âœ… Enterprise knowledge assistants (e.g. Azure Docs)
- âœ… High-throughput RAG pipelines
- âœ… Latency-sensitive LLM apps

# ğŸ§± Architecture Overview

```text
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚        User Query Input       â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ 1. Check Semantic Cache (FAISS)       â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Yes (high match)   â”‚ No (miss)
         â–¼                    â–¼
  Reuse Cached LLM     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      Response         â”‚ 2. Retrieve Context â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ 3. Build Prompt + Inject Docs  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ 4. Generate Response (Mistral LLM) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ 5. Postprocess + Store in Cache    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Key Modules

| Module | Purpose |
|--------|---------|
| `semantic_cache/embedder.py` | Loads BGE model and returns query embeddings |
| `semantic_cache/index_manager.py` | Manages FAISS index creation, loading, saving |
| `semantic_cache/operations.py` | Handles get/set/clear cache operations |
| `rag/retriever.py` |	Top-k document retrieval from Azure knowledge base |
| `rag/prompt_builder.py` |	Combines retrieved chunks + user question into LLM prompt |
| `rag/llm_client.py` |	Calls Mistral via Groq using LangChain |
| `rag/ingest_docs.py` |	Preprocesses and uploads local docs into FAISS vectorstore |
| `tests/` | Unit tests for all core functionality |

## ğŸš€ Usage (Example)

```python
from semantic_cache.operations import get_from_cache, set_in_cache

query = "top places to visit in France"
cached = get_from_cache(query)

if cached:
    print("âœ… Cache Hit:", cached)
else:
    response = "Paris, Lyon, Nice..."  
    set_in_cache(query, response)
```

## Run Tests
```python
pytest tests/
```


---
## ğŸ“Œ Next Steps
ğŸ” Add leaderboard and TTL/size-based cache trimming

ğŸ“š Ingest Azure PDF documentation automatically

ğŸŒ Wrap with FastAPI for API serving

â˜ï¸ Upgrade from FAISS â†’ Qdrant/Chroma

ğŸ¤– Migrate from Groq to AI Foundry (multi-LLM orchestration)

