# semantic-prompt-cache
This app leverages Semantic Caching to minimize inference latency and reduce API costs by reusing semantically similar prompt responses.
